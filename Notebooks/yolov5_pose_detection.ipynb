{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Mm8VtYGBNL"
      },
      "source": [
        "The YOLOv5 model for Object Detection is provided by [Ultralytics](https://github.com/ultralytics/yolov5), hence cloning the repository to run the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/yolov5"
      ],
      "metadata": {
        "id": "QCRmKtKZ76px"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNeJrdqXF-68",
        "outputId": "eeb5bf5d-3d82-447c-e330-1fcf4d7301a5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16413, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 16413 (delta 0), reused 3 (delta 0), pack-reused 16408\u001b[K\n",
            "Receiving objects: 100% (16413/16413), 14.90 MiB | 8.86 MiB/s, done.\n",
            "Resolving deltas: 100% (11265/11265), done.\n",
            "/content/yolov5\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # cloning Ultralytics repo\n",
        "%cd yolov5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/amalnairr/buddha-poses-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QERLx4KtIj-Y",
        "outputId": "10ef2e99-0789-4a17-a500-8894f069fd3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'buddha-poses-detection'...\n",
            "remote: Enumerating objects: 6133, done.\u001b[K\n",
            "remote: Counting objects: 100% (1489/1489), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1041/1041), done.\u001b[K\n",
            "remote: Total 6133 (delta 15), reused 1486 (delta 15), pack-reused 4644\u001b[K\n",
            "Receiving objects: 100% (6133/6133), 148.05 MiB | 27.50 MiB/s, done.\n",
            "Resolving deltas: 100% (867/867), done.\n",
            "Updating files: 100% (12359/12359), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AaqF4kzGGS5"
      },
      "source": [
        "The \"yolov5\" folder, or any git repo of this kind contains a text file called _requirements.txt_ which has a list of all the dependencies required to run the particular model. Here we proceed with installing the required dependencies for YOLOv5 and the other libraries, like Pytorch which are necessary for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy1hI3MrGFN3"
      },
      "outputs": [],
      "source": [
        "# installing dependencies as necessary\n",
        "!pip install -qr requirements.txt #ignore errors if any - rare but possible\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display plots and images\n",
        "from utils.downloads import attempt_download  # to download models/datasets\n",
        "\n",
        "# checking if connected to the GPU\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPoJ6vt1GwQK"
      },
      "source": [
        "The dataset here is from the [Warburg Institute Iconographic Database](https://iconographic.warburg.sas.ac.uk), and we have chosen a subsection of [Buddhist Sculptures](https://iconographic.warburg.sas.ac.uk/category/vpc-taxonomy-001747) for our project, and we will make attempts to detect buddha and his poses in the images of the sculptures using YOLOv5.\n",
        "\n",
        "- However YOLOv5 requires annotated dataset, and the annotation format is specific to the model. We used Roboflow to annotate and export the dataset in the required format, as a .zip file\n",
        "- What began with just annotation of the images into 7 classes, was then later, iteratively converted into a [dataset with 5 classes](https://universe.roboflow.com/warburg-buddha-poses/buddha-poses-detection/dataset/6). This last version has **Random Gaussian Blur** applied as an augementation method upfront, as the versions prior to the augmentation showed poorer performance.\n",
        "  - Update: The data was then converted to have [4 classes](https://universe.roboflow.com/warburg-buddha-poses/buddha-poses-detection/dataset/8) to address underrepresentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzDh8-kkGc0N"
      },
      "outputs": [],
      "source": [
        "# commenting out because cloning the repository on Colab for access\n",
        "# !rm -rf /content/yolov5/BuddhaPosesDataSet8\n",
        "# !unzip /content/BuddhaPosesDetection.v8.zip -d /content/yolov5/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ba3hlwcVdAk"
      },
      "source": [
        "The data.yaml file has the information regarding the train, test and validation datasets, the number of classes and their names."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_path = '/content/yolov5/buddha-poses-detection/buddha-detection-yolov5/version2-original/data.yaml'"
      ],
      "metadata": {
        "id": "PhTzwrVGJXr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQww25kTTed"
      },
      "outputs": [],
      "source": [
        "%cat {yaml_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9StBhGaNWCHe"
      },
      "source": [
        "**Commented for v8 dataset**\n",
        "\n",
        "Here it is evident that the dataset has 4 classes representing Buddha in 4 poses - Horse Riding (as Prince Siddhartha), Sitting, Standing, and Sleeping (while attaining Nirvana).\n",
        "\n",
        "- It is evident that 2 out of the 4 poses directly map to two different stages of Buddha's life, already."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxns6SZnWvZi"
      },
      "source": [
        "### Choice of Model and Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To5v_qJ6VNKo"
      },
      "outputs": [],
      "source": [
        "# define number of classes based on YAML - we already saw that the set has 4 classes in our case\n",
        "import yaml\n",
        "with open(yaml_path, 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ReSSdlXGKG"
      },
      "source": [
        "We will use the YOLOv5 - Small model for this project, as our idea is to use it for smartphones later on, hence the lighter, the better. Here are the model configuration parameters associated with YOLOv5s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm_85rTPW-Ug"
      },
      "outputs": [],
      "source": [
        "#this is the model configuration we will use for our project\n",
        "%cat /content/yolov5/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106B_cVYXkls"
      },
      "source": [
        "Here the number of classes defined are *80* while we only have **4**. We can choose to rewrite the file, as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3F603IlXVNt"
      },
      "outputs": [],
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzUHLo6oXzFP"
      },
      "outputs": [],
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMKcrrGwYGzk"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "Training the model is quite simple - just in terms of the code, and not the computational time, however. WE are using the Tesla T4 free GPU given by Google Colab for this case."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_directory = '/content/buddha-poses-detection/yolov5/buddha-poses-detection/buddha-detection-yolov5/version2-original'\n",
        "destination_directory = '/content/yolov5'\n",
        "\n",
        "# Copy the contents of the source directory to the destination directory\n",
        "shutil.copytree(source_directory, destination_directory, dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "id": "PfTH2fG8Oh_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4lf79QlX54L"
      },
      "outputs": [],
      "source": [
        "# train yolov5s on custom data for 100 epochs\n",
        "# time its performance\n",
        "%%time\n",
        "%cd /content/yolov5/\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/yolov5/data.yaml --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results_v2  --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVHCfwGYo90"
      },
      "source": [
        "The arguments parsed in the train program are:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n",
        "- **name:** result names\n",
        "- **nosave:** only save the final checkpoint\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a download link for the entire Results folder\n",
        "shutil.make_archive(\"/content/version1-original\", 'zip', \"/content/yolov5/runs/train/yolov5s_results_v2/\")\n",
        "\n",
        "# Rename the zip file to remove the \".zip\" extension\n",
        "shutil.move(\"/content/version1-original.zip\", \"/content/version2-original\")\n",
        "\n",
        "# Create a download link for the Results folder (unzipped)\n",
        "files.download(\"/content/version2-original\")\n"
      ],
      "metadata": {
        "id": "zSftZMEiVqX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the results:\n",
        "\n",
        "The model keeps a record of the above metrics, and stores them as results.txt. We can also plot the values of different metrics throughout the training process as follows:"
      ],
      "metadata": {
        "id": "Ckp2RgaW5H6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdbPhZJyYbuo"
      },
      "outputs": [],
      "source": [
        "from utils.plots import plot_results  # plot results.txt as results.png\n",
        "Image(filename='/content/yolov5/runs/train/yolov5s_results_v2/results.png', width=1000)  # view results.png"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image(filename='/content/yolov5/runs/train/yolov5s_results_v2/val_batch0_labels.jpg', width=900)"
      ],
      "metadata": {
        "id": "7H8GW1Rm5KzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out an augmented training example\n",
        "print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n",
        "Image(filename='/content/yolov5/runs/train/yolov5s_results_v2/train_batch0.jpg', width=900)"
      ],
      "metadata": {
        "id": "sikdVi4f58Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon going through the documentation, we could see that after training, a new folder named **runs** is created.\n",
        "\n",
        "Also, **what is the output of the training?** The output of training is a set of weights, iteratively optimized and stored at **best.pt**\n",
        "\n",
        "There is a script \"detect.py\" then uses the weights from the training to detect the different classes in the test dataset. Let us see if the model can detect the classes in the test images."
      ],
      "metadata": {
        "id": "E2qihHMe6Arv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the weights (output of train)\n",
        "%ls runs/train/yolov5s_results_v2/weights"
      ],
      "metadata": {
        "id": "hI3Lgb-558fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can either choose to run \"detect.py\" on one image, or the entire set of images\n",
        "%cd /content/yolov5/\n",
        "# testing on the entire test folder\n",
        "!python detect.py --weights runs/train/yolov5s_results_v2/weights/best.pt --img 416 --conf 0.4 --source test/images/"
      ],
      "metadata": {
        "id": "IkUNetma7PbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# displaying 10 random images post detection to show the result of the model\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg')[:10]: #assuming JPG\n",
        "    display(Image(filename=imageName))"
      ],
      "metadata": {
        "id": "IsnBVMap7MpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QNPTdG5A4Px"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}